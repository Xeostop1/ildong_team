from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime
import time
import csv


user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"
options = webdriver.ChromeOptions()
# options.add_argument("--headless")
options.add_argument("--window-size=1920,1080")
options.add_argument("user-agent={}".format(user_agent))
browser = webdriver.Chrome(
    service=Service(ChromeDriverManager().install()), options=options
)
browser.maximize_window()


search_keyword = "눈영양제"
start_date = "9/1/2022"
end_date = "6/11/2023"
loading_duration = 1
pages = 14

filename = f"{search_keyword}_구글_검색결과_{datetime.today().strftime('%Y_%m_%d')}.csv"
f = open(filename, "w", encoding="utf-8-sig", newline="")
writer = csv.writer(f)
title = "날짜,제목,내용".split(",")
writer.writerow(title)


result_container = []

for page in range(0, pages * 10, 10):
    url = f"https://www.google.com/search?q={search_keyword}&rlz=1C1JJTC_koKR1058KR1058&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}&ei=IKR4ZPzbApyk2roPz-2KmAs&start={page}&sa=N&ved=2ahUKEwj86-eynaL_AhUcklYBHc-2ArMQ8tMDegQIChAE&biw=1920&bih=947&dpr=1"

    browser.get(url)
    time.sleep(loading_duration)

    # if page == 10:
    #     time.sleep(20)

    soup = BeautifulSoup(browser.page_source, "lxml")
    search_results = soup.find("div", attrs={"id": "search"}).find_all(
        "div", attrs={"class": "g"}
    )

    page_number = int(page / 10 + 1)
    print("현재 페이지:", page_number)
    print("검색결과 수", len(search_results))
    print("#" * 70)

    for search_result in search_results:
        if search_result.select_one(".d4rhi"):
            sub_results = search_result.find_all(recursive=False)
            for sub_result in sub_results:
                result_container.append([sub_result, page_number])
        else:
            result_container.append([search_result, page_number])


for element in result_container:
    title = (
        element[0]
        .find("h3", attrs={"class": ["LC20lb MBeuO DKV0Md", "LC20lb MBeuO xvfwl"]})
        .get_text()
        .strip()
    )

    # content = element.find("span", class_="MUxGbd wuQ4Ob WZ8Tjf")
    content = element[0].find("div", class_="VwiC3b yXK7lf MUxGbd yDYNvb lyLwlc lEBKkf")
    if content:
        # content = content.next_sibling.get_text().strip()
        content = content.contents[1].get_text().strip()
    else:
        content = "내용 없음"

    date = element[0].find("span", attrs={"class": "MUxGbd wuQ4Ob WZ8Tjf"})
    if date:
        date = date.span.get_text().strip()
    else:
        date = "날짜 없음"

    print("제목:", title)
    print("내용:", content)
    print("날짜:", date)
    print("위치:", element[1])
    print("#" * 70)

    writer.writerow([date, title, content])


f.close()
browser.quit()
